
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta charset="utf-8" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Schedutil &#8212; The Linux Kernel 6.2.0-rc4+ documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Utilization Clamping" href="sched-util-clamp.html" />
    <link rel="prev" title="Energy Aware Scheduling" href="sched-energy.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="schedutil">
<h1>Schedutil<a class="headerlink" href="#schedutil" title="Permalink to this headline">¶</a></h1>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All this assumes a linear relation between frequency and work capacity,
we know this is flawed, but it is the best workable approximation.</p>
</div>
<section id="pelt-per-entity-load-tracking">
<h2>PELT (Per Entity Load Tracking)<a class="headerlink" href="#pelt-per-entity-load-tracking" title="Permalink to this headline">¶</a></h2>
<p>With PELT we track some metrics across the various scheduler entities, from
individual tasks to task-group slices to CPU runqueues. As the basis for this
we use an Exponentially Weighted Moving Average (EWMA), each period (1024us)
is decayed such that y^32 = 0.5. That is, the most recent 32ms contribute
half, while the rest of history contribute the other half.</p>
<p>Specifically:</p>
<blockquote>
<div><p>ewma_sum(u) := u_0 + u_1*y + u_2*y^2 + …</p>
<p>ewma(u) = ewma_sum(u) / ewma_sum(1)</p>
</div></blockquote>
<p>Since this is essentially a progression of an infinite geometric series, the
results are composable, that is ewma(A) + ewma(B) = ewma(A+B). This property
is key, since it gives the ability to recompose the averages when tasks move
around.</p>
<p>Note that blocked tasks still contribute to the aggregates (task-group slices
and CPU runqueues), which reflects their expected contribution when they
resume running.</p>
<p>Using this we track 2 key metrics: ‘running’ and ‘runnable’. ‘Running’
reflects the time an entity spends on the CPU, while ‘runnable’ reflects the
time an entity spends on the runqueue. When there is only a single task these
two metrics are the same, but once there is contention for the CPU ‘running’
will decrease to reflect the fraction of time each task spends on the CPU
while ‘runnable’ will increase to reflect the amount of contention.</p>
<p>For more detail see: kernel/sched/pelt.c</p>
</section>
<section id="frequency-cpu-invariance">
<h2>Frequency / CPU Invariance<a class="headerlink" href="#frequency-cpu-invariance" title="Permalink to this headline">¶</a></h2>
<p>Because consuming the CPU for 50% at 1GHz is not the same as consuming the CPU
for 50% at 2GHz, nor is running 50% on a LITTLE CPU the same as running 50% on
a big CPU, we allow architectures to scale the time delta with two ratios, one
Dynamic Voltage and Frequency Scaling (DVFS) ratio and one microarch ratio.</p>
<p>For simple DVFS architectures (where software is in full control) we trivially
compute the ratio as:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>          f_cur
r_dvfs := -----
          f_max
</pre></div>
</div>
<p>For more dynamic systems where the hardware is in control of DVFS we use
hardware counters (Intel APERF/MPERF, ARMv8.4-AMU) to provide us this ratio.
For Intel specifically, we use:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>         APERF
f_cur := ----- * P0
         MPERF

           4C-turbo;  if available and turbo enabled
f_max := { 1C-turbo;  if turbo enabled
           P0;        otherwise

                  f_cur
r_dvfs := min( 1, ----- )
                  f_max
</pre></div>
</div>
<p>We pick 4C turbo over 1C turbo to make it slightly more sustainable.</p>
<p>r_cpu is determined as the ratio of highest performance level of the current
CPU vs the highest performance level of any other CPU in the system.</p>
<blockquote>
<div><p>r_tot = r_dvfs * r_cpu</p>
</div></blockquote>
<p>The result is that the above ‘running’ and ‘runnable’ metrics become invariant
of DVFS and CPU type. IOW. we can transfer and compare them between CPUs.</p>
<p>For more detail see:</p>
<blockquote>
<div><ul class="simple">
<li><p>kernel/sched/pelt.h:update_rq_clock_pelt()</p></li>
<li><p>arch/x86/kernel/smpboot.c:”APERF/MPERF frequency ratio computation.”</p></li>
<li><p><a class="reference internal" href="sched-capacity.html"><span class="doc">Capacity Aware Scheduling</span></a>:”1. CPU Capacity + 2. Task utilization”</p></li>
</ul>
</div></blockquote>
</section>
<section id="util-est-util-est-fastup">
<h2>UTIL_EST / UTIL_EST_FASTUP<a class="headerlink" href="#util-est-util-est-fastup" title="Permalink to this headline">¶</a></h2>
<p>Because periodic tasks have their averages decayed while they sleep, even
though when running their expected utilization will be the same, they suffer a
(DVFS) ramp-up after they are running again.</p>
<p>To alleviate this (a default enabled option) UTIL_EST drives an Infinite
Impulse Response (IIR) EWMA with the ‘running’ value on dequeue – when it is
highest. A further default enabled option UTIL_EST_FASTUP modifies the IIR
filter to instantly increase and only decay on decrease.</p>
<p>A further runqueue wide sum (of runnable tasks) is maintained of:</p>
<blockquote>
<div><p>util_est := Sum_t max( t_running, t_util_est_ewma )</p>
</div></blockquote>
<p>For more detail see: kernel/sched/fair.c:util_est_dequeue()</p>
</section>
<section id="uclamp">
<h2>UCLAMP<a class="headerlink" href="#uclamp" title="Permalink to this headline">¶</a></h2>
<p>It is possible to set effective u_min and u_max clamps on each CFS or RT task;
the runqueue keeps an max aggregate of these clamps for all running tasks.</p>
<p>For more detail see: include/uapi/linux/sched/types.h</p>
</section>
<section id="schedutil-dvfs">
<h2>Schedutil / DVFS<a class="headerlink" href="#schedutil-dvfs" title="Permalink to this headline">¶</a></h2>
<p>Every time the scheduler load tracking is updated (task wakeup, task
migration, time progression) we call out to schedutil to update the hardware
DVFS state.</p>
<p>The basis is the CPU runqueue’s ‘running’ metric, which per the above it is
the frequency invariant utilization estimate of the CPU. From this we compute
a desired frequency like:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>           max( running, util_est );  if UTIL_EST
u_cfs := { running;                   otherwise

             clamp( u_cfs + u_rt , u_min, u_max );    if UCLAMP_TASK
u_clamp := { u_cfs + u_rt;                            otherwise

u := u_clamp + u_irq + u_dl;          [approx. see source for more detail]

f_des := min( f_max, 1.25 u * f_max )
</pre></div>
</div>
<p>XXX IO-wait: when the update is due to a task wakeup from IO-completion we
boost ‘u’ above.</p>
<p>This frequency is then used to select a P-state/OPP or directly munged into a
CPPC style request to the hardware.</p>
<p>XXX: deadline tasks (Sporadic Task Model) allows us to calculate a hard f_min
required to satisfy the workload.</p>
<p>Because these callbacks are directly from the scheduler, the DVFS hardware
interaction should be ‘fast’ and non-blocking. Schedutil supports
rate-limiting DVFS requests for when hardware interaction is slow and
expensive, this reduces effectiveness.</p>
<p>For more information see: kernel/sched/cpufreq_schedutil.c</p>
</section>
<section id="notes">
<h2>NOTES<a class="headerlink" href="#notes" title="Permalink to this headline">¶</a></h2>
<blockquote>
<div><ul class="simple">
<li><p>On low-load scenarios, where DVFS is most relevant, the ‘running’ numbers
will closely reflect utilization.</p></li>
<li><p>In saturated scenarios task movement will cause some transient dips,
suppose we have a CPU saturated with 4 tasks, then when we migrate a task
to an idle CPU, the old CPU will have a ‘running’ value of 0.75 while the
new CPU will gain 0.25. This is inevitable and time progression will
correct this. XXX do we still guarantee f_max due to no idle-time?</p></li>
<li><p>Much of the above is about avoiding DVFS dips, and independent DVFS domains
having to re-learn / ramp-up when load shifts.</p></li>
</ul>
</div></blockquote>
</section>
</section>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">The Linux Kernel</a></h1>



<p class="blurb">6.2.0-rc4-6.2.0-rc4+</p>







<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>
  <h3><a href="../index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Schedutil</a><ul>
<li><a class="reference internal" href="#pelt-per-entity-load-tracking">PELT (Per Entity Load Tracking)</a></li>
<li><a class="reference internal" href="#frequency-cpu-invariance">Frequency / CPU Invariance</a></li>
<li><a class="reference internal" href="#util-est-util-est-fastup">UTIL_EST / UTIL_EST_FASTUP</a></li>
<li><a class="reference internal" href="#uclamp">UCLAMP</a></li>
<li><a class="reference internal" href="#schedutil-dvfs">Schedutil / DVFS</a></li>
<li><a class="reference internal" href="#notes">NOTES</a></li>
</ul>
</li>
</ul>

  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="../_sources/scheduler/schedutil.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;The kernel development community.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 2.4.4</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/scheduler/schedutil.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>